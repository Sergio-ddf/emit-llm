{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sergio-ddf/emit-llm/blob/main/emit_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a650532",
      "metadata": {
        "id": "6a650532"
      },
      "source": [
        "# EMit Emotionâ€¯Detection Task"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Setup e import\n"
      ],
      "metadata": {
        "id": "cOLLaSlY0LTz"
      },
      "id": "cOLLaSlY0LTz"
    },
    {
      "cell_type": "code",
      "source": [
        "# API Key WandB\n",
        "from google.colab import userdata\n",
        "import os, wandb\n",
        "\n",
        "os.environ[\"WANDB_API_KEY\"] = userdata.get('WANDB_KEY')\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "2JIsZjXzOitS"
      },
      "id": "2JIsZjXzOitS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# installazione pacchetti e configurazione ambiente\n",
        "!pip install -q -U \"transformers>=4.39.0\" datasets scikit-multilearn emoji wandb\n",
        "!pip install -q iterative-stratification"
      ],
      "metadata": {
        "id": "5sd2RhQfoxDS"
      },
      "id": "5sd2RhQfoxDS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2 : import e seme di riproducibilitÃ \n",
        "import os, re, random, numpy as np, pandas as pd, emoji, torch\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
        "\n",
        "from datasets import Dataset, DatasetDict, Sequence, Value\n",
        "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
        "                          TrainingArguments, Trainer)\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True; torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "print(f\"PyTorch    : {torch.__version__} (CUDA disponibile: {torch.cuda.is_available()})\")"
      ],
      "metadata": {
        "id": "_FnIn-WBKIMs"
      },
      "id": "_FnIn-WBKIMs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Configurazione e percorsi\n"
      ],
      "metadata": {
        "id": "lE_rt2Cq1KMm"
      },
      "id": "lE_rt2Cq1KMm"
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR     = \"\"  # path della cartella con i dati\n",
        "MODEL_NAME   = \"Musixmatch/umberto-commoncrawl-cased-v1\"\n",
        "DEVICE       = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "LABELS       = ['Anger','Anticipation','Disgust','Fear','Joy',\n",
        "                'Love','Neutral','Sadness','Surprise','Trust']\n",
        "NUM_LABELS   = len(LABELS)\n"
      ],
      "metadata": {
        "id": "d0YyCzr61MS9"
      },
      "id": "d0YyCzr61MS9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Caricamento dati e statistiche\n"
      ],
      "metadata": {
        "id": "2u8AcObQ2fZO"
      },
      "id": "2u8AcObQ2fZO"
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(os.path.join(DATA_DIR, \"emit_train_A.csv\"))\n",
        "test_df = pd.read_csv(os.path.join(DATA_DIR, \"emit_test.csv\"))\n",
        "\n",
        "print(\"Train A:\", train_df.shape, \"Test in-domain:\", test_df.shape)\n",
        "display(train_df.head())\n",
        "\n",
        "# distribuzione etichette\n",
        "counts = train_df[LABELS].sum().sort_values(ascending=False)\n",
        "display(counts.to_frame(\"etichette\"))\n"
      ],
      "metadata": {
        "id": "zCEn-r9b2gMu"
      },
      "id": "zCEn-r9b2gMu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Pulizia testo\n"
      ],
      "metadata": {
        "id": "Jr9OwPMR2yAp"
      },
      "id": "Jr9OwPMR2yAp"
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5 : funzione clean + applicazione\n",
        "URL, USER, TAG = \"<URL>\", \"<USER>\", \"<HASHTAG>\"\n",
        "\n",
        "def clean(text: str) -> str:\n",
        "    \"\"\"Minimal text-normalizer per social media italiani.\"\"\"\n",
        "    t = re.sub(r'https?://\\S+', URL, text)          # link â†’ <URL>\n",
        "    t = re.sub(r'@\\w+', USER, t)                    # mention â†’ <USER>\n",
        "    t = re.sub(r'#(\\w+)', TAG + r' \\1', t)          # hashtag â†’ <HASHTAG> parola\n",
        "    t = emoji.demojize(t, language='it')            # ðŸ˜€ â†’ :grinning_face:\n",
        "    return t.strip()\n",
        "\n",
        "train_df[\"text_clean\"] = train_df[\"text\"].astype(str).map(clean)\n",
        "test_df[\"text_clean\"]  = test_df[\"text\"].astype(str).map(clean)\n"
      ],
      "metadata": {
        "id": "F6iA1c8b21KL"
      },
      "id": "F6iA1c8b21KL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Split stratificato 90/10\n"
      ],
      "metadata": {
        "id": "e0RoemCc3YtT"
      },
      "id": "e0RoemCc3YtT"
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6 : split multilabel\n",
        "X = train_df[\"text_clean\"].values\n",
        "Y = train_df[LABELS].values\n",
        "\n",
        "msss = MultilabelStratifiedShuffleSplit(test_size=0.1, random_state=SEED)\n",
        "train_idx, val_idx = next(msss.split(X, Y))\n",
        "\n",
        "train_split = train_df.iloc[train_idx].reset_index(drop=True)\n",
        "val_split   = train_df.iloc[val_idx].reset_index(drop=True)\n",
        "\n",
        "print(f\"â†’ Train {train_split.shape}  Valid {val_split.shape}\")\n"
      ],
      "metadata": {
        "id": "ipo8JMGZ3xGT"
      },
      "id": "ipo8JMGZ3xGT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Baseline TF-IDF + Logistic Regression\n"
      ],
      "metadata": {
        "id": "KKp7lPxE4dqM"
      },
      "id": "KKp7lPxE4dqM"
    },
    {
      "cell_type": "code",
      "source": [
        "vec = TfidfVectorizer(ngram_range=(1,2), max_features=5000)\n",
        "X_tr = vec.fit_transform(train_split['text_clean'])\n",
        "X_va = vec.transform(val_split['text_clean'])\n",
        "\n",
        "y_tr = train_split[LABELS].values\n",
        "y_va = val_split[LABELS].values\n",
        "\n",
        "clf = OneVsRestClassifier(LogisticRegression(max_iter=1000, random_state=42))\n",
        "clf.fit(X_tr, y_tr)\n",
        "y_pr = clf.predict(X_va)\n",
        "\n",
        "print(\"TF-IDF+LR macro-F1:\", f1_score(y_va, y_pr, average='macro'))\n",
        "print(classification_report(y_va, y_pr, target_names=LABELS, zero_division=0))\n"
      ],
      "metadata": {
        "id": "AXlI1fnT4k-h"
      },
      "id": "AXlI1fnT4k-h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Preparazione HF Dataset\n"
      ],
      "metadata": {
        "id": "feZB_8JS52aI"
      },
      "id": "feZB_8JS52aI"
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7 (aggiornata) â€” cast delle labels a float32\n",
        "model_ckpt = \"Musixmatch/umberto-commoncrawl-cased-v1\"\n",
        "tokenizer  = AutoTokenizer.from_pretrained(model_ckpt, use_fast=True)\n",
        "\n",
        "# Prepara la colonna labels come lista di float\n",
        "for df in (train_split, val_split, test_df):\n",
        "    df[\"labels\"] = df[LABELS].apply(lambda row: [float(x) for x in row], axis=1)\n",
        "\n",
        "def tokenize(batch):\n",
        "    tokens = tokenizer(\n",
        "        batch[\"text_clean\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    )\n",
        "    # qui batch[\"labels\"] Ã¨ giÃ  lista di float\n",
        "    tokens[\"labels\"] = batch[\"labels\"]\n",
        "    return tokens\n",
        "\n",
        "def to_ds(df):\n",
        "    ds = Dataset.from_pandas(df[[\"text_clean\", \"labels\"]])\n",
        "    # assicurati che lo schema di labels sia float32\n",
        "    ds = ds.cast_column(\"labels\", Sequence(feature=Value(\"float32\")))\n",
        "    return ds.map(tokenize, batched=True)\n",
        "\n",
        "hf_train = to_ds(train_split)\n",
        "hf_val   = to_ds(val_split)\n",
        "hf_test  = to_ds(test_df)\n",
        "\n",
        "data = DatasetDict({\"train\": hf_train, \"validation\": hf_val, \"test\": hf_test})\n",
        "data = data.remove_columns([\"text_clean\"])\n"
      ],
      "metadata": {
        "id": "2DjxtAIZ53Ib"
      },
      "id": "2DjxtAIZ53Ib",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Fine-tuning UmBERTo con BCE pesata\n"
      ],
      "metadata": {
        "id": "VDjvM3Cu6iCL"
      },
      "id": "VDjvM3Cu6iCL"
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8 â€“ corretta per CamembertClassificationHead (UmBERTo)\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "# 1) Carichiamo il modello base\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_ckpt,\n",
        "    num_labels=NUM_LABELS,\n",
        "    problem_type=\"multi_label_classification\"\n",
        ")\n",
        "\n",
        "# 2) Calcolo del bias iniziale come log(pi / (1 - pi))\n",
        "label_sums = train_split[LABELS].sum()\n",
        "pi = label_sums / len(train_split)\n",
        "pi_tensor = torch.tensor(pi.values, dtype=torch.float32)\n",
        "bias_init = torch.log(pi_tensor / (1.0 - pi_tensor + 1e-8))\n",
        "\n",
        "# 3) Inizializzazione del bias del layer finale\n",
        "model.classifier.out_proj.bias.data = bias_init.to(model.device)\n",
        "\n",
        "# 4) Calcolo pos_weight per classi sbilanciate\n",
        "neg_counts = len(train_split) - label_sums\n",
        "pos_weight_np = (neg_counts / label_sums).values\n",
        "pos_weight_tensor = torch.tensor(pos_weight_np, dtype=torch.float32, device=model.device)\n"
      ],
      "metadata": {
        "id": "_7Ueqtp66jES"
      },
      "id": "_7Ueqtp66jES",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9 â€“ hyperparametri + Warmup + Scheduler\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir               = \"./ubert-emotions\",\n",
        "    eval_strategy            = \"epoch\",\n",
        "    save_strategy            = \"epoch\",\n",
        "    logging_strategy         = \"steps\",\n",
        "    logging_steps            = 100,\n",
        "    learning_rate            = 2e-5,\n",
        "    per_device_train_batch_size = 16,\n",
        "    per_device_eval_batch_size  = 32,\n",
        "    num_train_epochs         = 5,\n",
        "    weight_decay             = 0.01,\n",
        "    warmup_ratio             = 0.1,            # 10% dei steps di warmup\n",
        "    lr_scheduler_type        = \"cosine\",       # cosine decay\n",
        "    metric_for_best_model    = \"eval_f1\",\n",
        "    load_best_model_at_end   = True,\n",
        "    seed                     = SEED,\n",
        "    report_to                = [\"wandb\"],\n",
        ")\n"
      ],
      "metadata": {
        "id": "AoTeHPjw7x6O"
      },
      "id": "AoTeHPjw7x6O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10 â€“ metrica + CustomTrainer con pos_weight (fix device)\n",
        "\n",
        "import torch.nn as nn\n",
        "from transformers import Trainer\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# 1) compute_metrics invariata\n",
        "def compute_metrics(pred):\n",
        "    logits, labels = pred\n",
        "    probs  = torch.sigmoid(torch.tensor(logits))\n",
        "    y_pred = (probs > 0.5).int().cpu().numpy()\n",
        "    y_true = labels\n",
        "    return {\"f1\": f1_score(y_true, y_pred, average=\"macro\")}\n",
        "\n",
        "# 2) CustomTrainer â€“ spostiamo pos_weight su GPU dentro compute_loss\n",
        "class CustomTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        # Estrai le labels e manda tutto su device\n",
        "        labels = inputs.pop(\"labels\").float().to(model.device)\n",
        "        # Forward pass\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        # Sposta pos_weight_tensor sullo stesso device di logits\n",
        "        pw = pos_weight_tensor.to(logits.device)\n",
        "        # BCEWithLogits con pos_weight corretto\n",
        "        loss_fct = nn.BCEWithLogitsLoss(pos_weight=pw)\n",
        "        loss = loss_fct(logits, labels)\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# 3) instanzia e lancia il training come prima\n",
        "trainer = CustomTrainer(\n",
        "    model           = model,\n",
        "    args            = args,\n",
        "    train_dataset   = data[\"train\"],\n",
        "    eval_dataset    = data[\"validation\"],\n",
        "    tokenizer       = tokenizer,\n",
        "    compute_metrics = compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "XKWDd51k7gCG"
      },
      "id": "XKWDd51k7gCG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11 â€“ Calibrazione delle soglie su validation\n",
        "# 1) Otteniamo le probabilitÃ  predette sulla validation\n",
        "val_logits = trainer.predict(data[\"validation\"]).predictions\n",
        "val_probs  = torch.sigmoid(torch.tensor(val_logits)).cpu().numpy()\n",
        "\n",
        "# 2) Preleviamo le vere etichette dal DataFrame val_split\n",
        "y_val_true = val_split[LABELS].values\n",
        "\n",
        "# 3) Ricerca soglia ottimale per ciascuna classe\n",
        "best_thresholds = {}\n",
        "for i, label in enumerate(LABELS):\n",
        "    best_f1, best_thr = 0.0, 0.5\n",
        "    for thr in np.arange(0.1, 0.9, 0.01):\n",
        "        y_pred_i = (val_probs[:, i] > thr).astype(int)\n",
        "        f1 = f1_score(y_val_true[:, i], y_pred_i)\n",
        "        if f1 > best_f1:\n",
        "            best_f1, best_thr = f1, thr\n",
        "    best_thresholds[label] = best_thr\n",
        "\n",
        "print(\"Soglie ottimali per classe:\")\n",
        "for lbl, thr in best_thresholds.items():\n",
        "    print(f\"  {lbl:12s}: {thr:.2f}\")\n"
      ],
      "metadata": {
        "id": "lME-9w240wUX"
      },
      "id": "lME-9w240wUX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12 â€“ Inference sul test con soglie calibrate\n",
        "# 1) ProbabilitÃ  sul test set\n",
        "test_logits = trainer.predict(data[\"test\"]).predictions\n",
        "test_probs  = torch.sigmoid(torch.tensor(test_logits)).cpu().numpy()\n",
        "\n",
        "# 2) Applichiamo le soglie ottimali\n",
        "threshold_list = np.array([best_thresholds[l] for l in LABELS])\n",
        "binary_preds   = (test_probs > threshold_list).astype(int)\n",
        "\n",
        "# 3) Costruiamo la submission\n",
        "submission = pd.DataFrame({\n",
        "    \"id\": test_df[\"id\"],\n",
        "    \"labels\": [\n",
        "        \" \".join([lbl for lbl, flag in zip(LABELS, row) if flag])\n",
        "        if row.sum() > 0 else \"Neutral\"\n",
        "        for row in binary_preds\n",
        "    ]\n",
        "})\n",
        "\n",
        "submission.to_csv(\"emit_submission_calibrated.csv\", index=False)\n",
        "print(\"CSV salvato in emit_submission_calibrated.csv\")\n"
      ],
      "metadata": {
        "id": "dAZ2Kd3nsOA9"
      },
      "id": "dAZ2Kd3nsOA9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 13 â€“ Valutazione del modello sul CSV di submission\n",
        "\n",
        "# Percorsi\n",
        "SUBM_PATH = \"emit_submission_calibrated.csv\"\n",
        "TRUE_TEST = \"emit_test.csv\"  # il CSV di test in-domain con le etichette vere\n",
        "\n",
        "# 1) Carica submission e ground truth\n",
        "subm_df = pd.read_csv(SUBM_PATH)\n",
        "true_df = pd.read_csv(TRUE_TEST)\n",
        "\n",
        "# 2) Costruisci y_true e y_pred come matrici binarie [n_examples Ã— 10]\n",
        "#    y_true: direttamente dalle colonne LABELS di true_df\n",
        "y_true = true_df[LABELS].values.astype(int)\n",
        "\n",
        "#    y_pred: da subm_df[\"labels\"] (stringhe tipo \"Joy Love\" o \"Neutral\")\n",
        "def parse_labels(label_str):\n",
        "    if label_str.strip() == \"Neutral\":\n",
        "        # vettore tutto zero tranne Neutral=1\n",
        "        vec = np.zeros(len(LABELS), dtype=int)\n",
        "        vec[LABELS.index(\"Neutral\")] = 1\n",
        "        return vec\n",
        "    toks = label_str.split()\n",
        "    vec = np.array([1 if lbl in toks else 0 for lbl in LABELS], dtype=int)\n",
        "    return vec\n",
        "\n",
        "y_pred = np.vstack(subm_df[\"labels\"].map(parse_labels).values)\n",
        "\n",
        "# 3) Calcola le metriche\n",
        "print(\"Macro-F1 generale:\", f1_score(y_true, y_pred, average=\"macro\"))\n",
        "print(\"\\nClassification Report (per classe):\\n\")\n",
        "print(classification_report(y_true, y_pred, target_names=LABELS, digits=4))\n"
      ],
      "metadata": {
        "id": "Y2qpd6jx6dC1"
      },
      "id": "Y2qpd6jx6dC1",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}