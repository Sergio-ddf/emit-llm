{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sergio-ddf/emit-llm/blob/main/emit_setup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a650532",
      "metadata": {
        "id": "6a650532"
      },
      "source": [
        "# EMit Emotionâ€¯Detection Task"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Setup e import\n"
      ],
      "metadata": {
        "id": "cOLLaSlY0LTz"
      },
      "id": "cOLLaSlY0LTz"
    },
    {
      "cell_type": "code",
      "source": [
        "# API Key WandB\n",
        "from google.colab import userdata\n",
        "import os, wandb\n",
        "\n",
        "os.environ[\"WANDB_API_KEY\"] = userdata.get('WANDB_KEY')\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "2JIsZjXzOitS"
      },
      "id": "2JIsZjXzOitS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Installazione delle dipendenze\n",
        "!pip install -q datasets emoji iterative-stratification evaluate tokenizers\n"
      ],
      "metadata": {
        "id": "5xpwet2RKElx"
      },
      "id": "5xpwet2RKElx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import principali e informazioni sull'ambiente\n",
        "import os\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import emoji\n",
        "\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "from torch.nn.functional import sigmoid\n",
        "\n",
        "print(f\"PyTorch    : {torch.__version__} (CUDA disponibile: {torch.cuda.is_available()})\")"
      ],
      "metadata": {
        "id": "_FnIn-WBKIMs"
      },
      "id": "_FnIn-WBKIMs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Configurazione e percorsi\n"
      ],
      "metadata": {
        "id": "lE_rt2Cq1KMm"
      },
      "id": "lE_rt2Cq1KMm"
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR     = \"\"  # path della cartella con i dati\n",
        "MODEL_NAME   = \"Musixmatch/umberto-commoncrawl-cased-v1\"\n",
        "DEVICE       = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "LABELS       = ['Anger','Anticipation','Disgust','Fear','Joy',\n",
        "                'Love','Neutral','Sadness','Surprise','Trust']\n",
        "NUM_LABELS   = len(LABELS)\n"
      ],
      "metadata": {
        "id": "d0YyCzr61MS9"
      },
      "id": "d0YyCzr61MS9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Caricamento dati e statistiche\n"
      ],
      "metadata": {
        "id": "2u8AcObQ2fZO"
      },
      "id": "2u8AcObQ2fZO"
    },
    {
      "cell_type": "code",
      "source": [
        "train_a = pd.read_csv(os.path.join(DATA_DIR, \"emit_train_A.csv\"))\n",
        "test_in = pd.read_csv(os.path.join(DATA_DIR, \"emit_test.csv\"))\n",
        "\n",
        "print(\"Train A:\", train_a.shape, \"Test in-domain:\", test_in.shape)\n",
        "display(train_a.head())\n",
        "\n",
        "# distribuzione etichette\n",
        "counts = train_a[LABELS].sum().sort_values(ascending=False)\n",
        "display(counts.to_frame(\"etichette\"))\n"
      ],
      "metadata": {
        "id": "zCEn-r9b2gMu"
      },
      "id": "zCEn-r9b2gMu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Pulizia testo\n"
      ],
      "metadata": {
        "id": "Jr9OwPMR2yAp"
      },
      "id": "Jr9OwPMR2yAp"
    },
    {
      "cell_type": "code",
      "source": [
        "URL, USER, TAG = \"<URL>\", \"<USER>\", \"<HASHTAG>\"\n",
        "\n",
        "def clean(text: str) -> str:\n",
        "    t = re.sub(r'https?://\\S+', URL, text)\n",
        "    t = re.sub(r'@\\w+', USER, t)\n",
        "    t = re.sub(r'#(\\w+)', TAG + r' \\1', t)\n",
        "    t = emoji.demojize(text, language='it')\n",
        "    return t.strip()\n",
        "\n",
        "train_a['text_clean'] = train_a['text'].map(clean)\n",
        "train_a[['text', 'text_clean']].head()\n",
        "\n"
      ],
      "metadata": {
        "id": "F6iA1c8b21KL"
      },
      "id": "F6iA1c8b21KL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Split stratificato 90/10\n"
      ],
      "metadata": {
        "id": "e0RoemCc3YtT"
      },
      "id": "e0RoemCc3YtT"
    },
    {
      "cell_type": "code",
      "source": [
        "X = train_a['text_clean'].values\n",
        "Y = train_a[LABELS].values\n",
        "\n",
        "msss = MultilabelStratifiedShuffleSplit(test_size=0.1, random_state=42)\n",
        "train_idx, val_idx = next(msss.split(X, Y))\n",
        "\n",
        "train_df = train_a.iloc[train_idx].reset_index(drop=True)\n",
        "val_df   = train_a.iloc[val_idx].reset_index(drop=True)\n",
        "\n",
        "print(\"Train/Val:\", train_df.shape[0], \"/\", val_df.shape[0])\n"
      ],
      "metadata": {
        "id": "ipo8JMGZ3xGT"
      },
      "id": "ipo8JMGZ3xGT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Baseline TF-IDF + Logistic Regression\n"
      ],
      "metadata": {
        "id": "KKp7lPxE4dqM"
      },
      "id": "KKp7lPxE4dqM"
    },
    {
      "cell_type": "code",
      "source": [
        "vec = TfidfVectorizer(ngram_range=(1,2), max_features=5000)\n",
        "X_tr = vec.fit_transform(train_df['text_clean'])\n",
        "X_va = vec.transform(val_df['text_clean'])\n",
        "\n",
        "y_tr = train_df[LABELS].values\n",
        "y_va = val_df[LABELS].values\n",
        "\n",
        "clf = OneVsRestClassifier(LogisticRegression(max_iter=1000, random_state=42))\n",
        "clf.fit(X_tr, y_tr)\n",
        "y_pr = clf.predict(X_va)\n",
        "\n",
        "print(\"TF-IDF+LR macro-F1:\", f1_score(y_va, y_pr, average='macro'))\n",
        "print(classification_report(y_va, y_pr, target_names=LABELS, zero_division=0))\n"
      ],
      "metadata": {
        "id": "AXlI1fnT4k-h"
      },
      "id": "AXlI1fnT4k-h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Preparazione HF Dataset\n"
      ],
      "metadata": {
        "id": "feZB_8JS52aI"
      },
      "id": "feZB_8JS52aI"
    },
    {
      "cell_type": "code",
      "source": [
        "# colonne labels in float vector\n",
        "for df in (train_df, val_df):\n",
        "    df['labels'] = df[LABELS].astype(float).values.tolist()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "MAX_LEN   = 128\n",
        "\n",
        "def tokenize_fn(batch):\n",
        "    enc = tokenizer(batch['text_clean'],\n",
        "                    padding='max_length',\n",
        "                    truncation=True,\n",
        "                    max_length=MAX_LEN)\n",
        "    enc['labels'] = batch['labels']\n",
        "    return enc\n",
        "\n",
        "drop_cols = ['text', 'text_clean'] + LABELS\n",
        "\n",
        "dtrain = Dataset.from_pandas(train_df)\\\n",
        "           .map(tokenize_fn, batched=True, remove_columns=drop_cols)\\\n",
        "           .with_format('torch', columns=['input_ids','attention_mask','labels'])\n",
        "dval   = Dataset.from_pandas(val_df)\\\n",
        "           .map(tokenize_fn, batched=True, remove_columns=drop_cols)\\\n",
        "           .with_format('torch', columns=['input_ids','attention_mask','labels'])\n"
      ],
      "metadata": {
        "id": "2DjxtAIZ53Ib"
      },
      "id": "2DjxtAIZ53Ib",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Fine-tuning UmBERTo con BCE pesata\n"
      ],
      "metadata": {
        "id": "VDjvM3Cu6iCL"
      },
      "id": "VDjvM3Cu6iCL"
    },
    {
      "cell_type": "code",
      "source": [
        "# pesi per BCE\n",
        "freqs       = train_df[LABELS].mean().values\n",
        "pos_weights = torch.tensor((1 - freqs) / freqs, device=DEVICE)\n",
        "\n",
        "class WeightedTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        inputs.pop(\"num_items_in_batch\", None)\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        logits = model(**inputs).logits\n",
        "        loss   = BCEWithLogitsLoss(pos_weight=pos_weights)(logits, labels.float())\n",
        "        return (loss, logits) if return_outputs else loss\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=NUM_LABELS,\n",
        "    problem_type='multi_label_classification'\n",
        ").to(DEVICE)\n",
        "model.config.id2label = {i:l for i,l in enumerate(LABELS)}\n",
        "model.config.label2id = {l:i for i,l in enumerate(LABELS)}\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir='ckpt/umberto',\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=10,\n",
        "    lr_scheduler_type='linear',\n",
        "    warmup_steps=500,\n",
        "    logging_steps=50,\n",
        "    eval_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='eval_macro_f1',\n",
        "    report_to=None\n",
        ")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    probs  = sigmoid(torch.tensor(logits))\n",
        "    preds  = (probs >= 0.5).int().numpy()\n",
        "    return {\"eval_macro_f1\": float(f1_score(labels, preds, average='macro', zero_division=0))}\n",
        "\n",
        "trainer = WeightedTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=dtrain,\n",
        "    eval_dataset=dval,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n"
      ],
      "metadata": {
        "id": "_7Ueqtp66jES"
      },
      "id": "_7Ueqtp66jES",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "AoTeHPjw7x6O"
      },
      "id": "AoTeHPjw7x6O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Ottimizzazione soglie e valutazione finale\n"
      ],
      "metadata": {
        "id": "DyhyGclB7fUp"
      },
      "id": "DyhyGclB7fUp"
    },
    {
      "cell_type": "code",
      "source": [
        "# logits e labels\n",
        "preds_out = trainer.predict(dval)\n",
        "val_probs = sigmoid(torch.tensor(preds_out.predictions)).numpy()\n",
        "val_labels= val_df[LABELS].values.astype(int)\n",
        "\n",
        "# soglie per classe\n",
        "best_t = []\n",
        "for i in range(NUM_LABELS):\n",
        "    best_f1, best_thresh = 0, 0.5\n",
        "    for t in np.linspace(0.1, 0.9, 81):\n",
        "        f1 = f1_score(val_labels[:,i], (val_probs[:,i]>=t).astype(int))\n",
        "        if f1>best_f1:\n",
        "            best_f1, best_thresh = f1, t\n",
        "    best_t.append(best_thresh)\n",
        "\n",
        "print(dict(zip(LABELS, best_t)))\n",
        "\n",
        "# macro-F1 ottimizzata\n",
        "preds_opt = (val_probs >= np.array(best_t)).astype(int)\n",
        "print(\"Final optimized macro-F1:\", f1_score(val_labels, preds_opt, average='macro', zero_division=0))\n"
      ],
      "metadata": {
        "id": "XKWDd51k7gCG"
      },
      "id": "XKWDd51k7gCG",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}